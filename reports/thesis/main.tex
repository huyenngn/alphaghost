\documentclass[12pt,oneside,openright]{article}
\usepackage{booktabs}
%Document Variables
\newcommand{\topic}
{Reinforcement Learning outperforms pure Monte Carlo Tree Search approaches in Phantom Go}

\usepackage[utf8]{inputenc}
\usepackage[scaled]{helvet}
\renewcommand\familydefault{\sfdefault} 
\usepackage[T1]{fontenc}
\usepackage{fancyhdr,xcolor}
\usepackage{xcolor}
\usepackage{datetime2}
\usepackage{biblatex}
\usepackage{float}
\usepackage{hyperref}
\usepackage{tabularx}
\hypersetup{
    colorlinks=false,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
  a4paper,
  left=30mm,
  right=30mm,
  top=4.5cm,
  headheight=4cm,
  bottom=4.5cm,
  footskip=3cm
}
\usepackage{easyReview}

\renewcommand*{\bibfont}{\footnotesize}
\addbibresource{sample.bib}
\usepackage{xurl}
\newcommand{\changefont}{%
    \fontsize{23}{26}\selectfont
}
\definecolor{boxcl}{HTML}{666666}
\definecolor{tubred}{HTML}{c50e1f}

\graphicspath{{assets/}}
\let\oldheadrule\headrule
\renewcommand{\headrule}{\color{tubred}\oldheadrule}
\renewcommand{\headrulewidth}{1.5pt}
\fancyfoot{}

\setlength{\parindent}{0pt}

\fancyhead[HL]{Bachelor's Thesis Proposal} 
\fancyhead[HR]{\includegraphics[width=0.15\textwidth]{assets/TUB.png}}
\fancyfoot[R]{\centering\thepage}
\pagestyle{fancy}
\begin{document}
\date{\today}
% \begin{titlepage}
\begin{center}
    \vspace*{1cm}
    \Huge
    \textbf{\topic}
    \LARGE
    %Thesis Subtitle

    \vspace{2cm}

    \textbf{Thi Nguyen Ngan Huyen}\\
    \vspace{0.5cm}
    \normalsize
    Matr Nr.: 400883\\
    E-mail: thinguyen@campus.tu-berlin.de
    \vspace{2cm}

    \large
    \begin{center}
        First Supervisor: Prof. Dr. Dr. h.c. Sahin Albayrak \\
        Second Supervisor: Dr.- Ing. Stefan Fricke
    \end{center}        \vspace{0.8cm}

    \Large
    Technische Universität Berlin\\
    Fakultät IV – Elektrotechnik und Informatik\\
    \vspace{1cm}
    \today
\end{center}
\newpage
% \end{titlepage}
\pagenumbering{arabic}

\section{Introduction}

Phantom Go is a variant of Go in which part of the information is hidden from the players, making it a game of imperfect information. Unlike standard Go, which has been practically solved by AIs such as AlphaZero, limited research has been conducted on Phantom Go due to its hidden state mechanics. Players only see their own stones and not the opponent’s moves, requiring AI to make decisions based on inference and probabilistic reasoning rather than deterministic board evaluations.

\subsection{Reinforcement Learning in Go}

The state-of-the-art for standard Go is dominated by neural networks (NN) and reinforcement learning (RL), as exemplified by AlphaGo (Silver et al., 2016)\cite{Silver2016} and AlphaZero (Silver et al., 2017)\cite{Silver2017}, which have demonstrated superhuman performance in Go and other board games, outperforming human experts and traditional AI methods.

These systems:
\begin{itemize}
    \item Utilize deep neural networks (DNN) to evaluate board states and predict optimal moves.
    \item Train through self-play using reinforcement learning to optimize strategies.
    \item Combine Monte Carlo Tree Search (MCTS) with neural networks to guide rollouts and improve decision-making.
\end{itemize}

\subsection{State of research for Phantom Go}

Unlike standard Go, Phantom Go introduces hidden information, requiring AI agents to infer the opponent’s moves and board state.

Existing approaches extend MCTS with a belief model that simulates potential opponent stone placements, such as the one described by Cazenave in 2006\cite{Cazenave2006}.
It tracks information revealed from illegal move requests and captures, then randomly assigns unknown opponent stones rather than inferring strategic placement. From 10,000 random games, the bot plays the move with the highest win rate.

While this approach claims to have achieved intermediate-level play, it does not accurately capture the true belief state and likely struggles with long-term strategy and fails to exploit patterns or opponent mistakes effectively.

To address these limitations, this thesis proposes to train a neural network to predict opponent moves and likely board states. By leveraging RL to guide MCTS rollouts, similar to AlphaZero but adapted for imperfect information, this approach aims to improve strategic decision-making under uncertainty.

\section{Research Question}

This thesis investigates the hypothesis:
\\
\\
\textbf{Does RL-enhanced MCTS outperform pure MCTS in Phantom Go?}
\\
\\
To answer this question, the research will include developing an MCTS bot that integrates NN predictions and conducting experiments comparing RL-enhanced MCTS to pure MCTS-based approaches with random rollouts.


\section{Thesis Approach}


\subsection{RL-Enhanced MCTS}

To address the limitations of pure MCTS, this thesis proposes to combine RL with MCTS, following a paradigm similar to AlphaZero but adapted for imperfect information.

\begin{enumerate}
    \item Neural Network for State Evaluation: Instead of relying on random rollouts, the AI will train a DNN to predict the value of board positions and suggest plausible opponent moves.
    \item Self-Play Training: The model will be trained through self-play, using Proximal Policy Optimization (PPO) as an RL method to optimize move selection.
    \item Integration with MCTS: The learned value function will replace random rollouts in MCTS, making search simulations more efficient and accurate.
\end{enumerate}

Besides a Phantom Go environment, which will be used for game simulation, Google Deepmind's OpenSpiel framework also provides a variety of algorithms for reinforcement learning, including MCTS and PPO, which can be used to train the RL agent. It may be harder to modify for custom inputs, in which case PyTorch or Stable-Baselines3 will be used for more flexibility.

\subsection{Evaluation: Comparing RL-enhanced MCTS vs. pure MCTS-Based Approaches}

The evaluation will compare the RL-enhanced MCTS approach against a baseline, which is Google's OpenSpiel MCTS bot for Phantom Go. The metrics for comparison will include the win rate against the pure MCTS baseline, the average game length, and the convergence speed.


\section{Timeline}

\begin{table}[H]
    \caption{Timeline}
    \centering
    \begin{tabularx}{\textwidth}{X|X|l}
        \toprule
        \multicolumn{1}{c}{Phase}     &
        \multicolumn{1}{c}{Task}      &
        \multicolumn{1}{c}{Time}                                                                                          \\
        \midrule
        1. Literature Review          & Survey existing Phantom Go AI research, MCTS, AlphaZero and RL papers.  & 2 weeks \\
        \hline
        2. Technology Familiarization & Study OpenSpiel documentation (optional: PyTorch, Stable-Baselines3)    & 2 weeks \\
        \hline
        3. Implementation             & Implement PPO-based training for MCTS agent.                            & 3 weeks \\
        \hline
        4. Compare Against Baseline   & Evaluate AI performance against OpenSpiel MCTS bot with random rollouts & 1 week  \\
        \hline
        6. Analyze Results            & Compare strategy effectiveness, tune hyperparameters.                   & 2 week  \\
        \hline
        7. Thesis Writing             & Document research, methodology, results, and conclusions                & 3 weeks \\
        \hline
    \end{tabularx}
    \label{tab:timeline}
\end{table}


\section{References}
\printbibliography[heading=none]

\end{document}
