@Article{Silver2016alphago,
author={Silver, David
and Huang, Aja
and Maddison, Chris J.
and Guez, Arthur
and Sifre, Laurent
and van den Driessche, George
and Schrittwieser, Julian
and Antonoglou, Ioannis
and Panneershelvam, Veda
and Lanctot, Marc
and Dieleman, Sander
and Grewe, Dominik
and Nham, John
and Kalchbrenner, Nal
and Sutskever, Ilya
and Lillicrap, Timothy
and Leach, Madeleine
and Kavukcuoglu, Koray
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go with deep neural networks and tree search},
journal={Nature},
year={2016},
month={01},
volume={529},
number={7587},
pages={484-489},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
issn={1476-4687},
doi={10.1038/nature16961},
url={https://doi.org/10.1038/nature16961}
}

@Article{Silver2017alphazero,
author={Silver, David
and Schrittwieser, Julian
and Simonyan, Karen
and Antonoglou, Ioannis
and Huang, Aja
and Guez, Arthur
and Hubert, Thomas
and Baker, Lucas
and Lai, Matthew
and Bolton, Adrian
and Chen, Yutian
and Lillicrap, Timothy
and Hui, Fan
and Sifre, Laurent
and van den Driessche, George
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
month={10},
volume={550},
number={7676},
pages={354-359},
abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
issn={1476-4687},
doi={10.1038/nature24270},
url={https://doi.org/10.1038/nature24270}
}

@InProceedings{Cazenave2006phantomGo,
author="Cazenave, Tristan",
editor="van den Herik, H. Jaap
and Hsu, Shun-Chin
and Hsu, Tsan-sheng
and Donkers, H. H. L. M. (Jeroen)",
title="A Phantom-Go Program",
booktitle="Advances in Computer Games",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="120--125",
abstract="This paper discusses the intricacies of a Phantom-Go program. It is based on a Monte-Carlo approach. The program called Illusion plays Phantom Go at an intermediate level. The emphasis is on strategies, tactical search, and specialized knowledge. The paper provides a better understanding of the fundamentals of Monte-Carlo search in Go.",
isbn="978-3-540-48889-7",
doi={10.1007/11922155_9}
}

@article{Cowling2012ismcts,
author = {Cowling, Peter and Powley, Edward and Whitehouse, Daniel},
year = {2012},
month = {06},
pages = {120-143},
title = {Information Set Monte Carlo Tree Search},
volume = {4},
journal = {IEEE Transactions on Computational Intelligence and Ai in Games},
doi = {10.1109/TCIAIG.2012.2200894}
}

@INPROCEEDINGS{Wang2015bsmcts,
  author={Wang, Jiao and Zhu, Tan and Li, Hongye and Hsueh, Chu-Hsuan and Wu, I-Chen},
  booktitle={2015 IEEE Conference on Computational Intelligence and Games (CIG)}, 
  title={Belief-state Monte-Carlo tree search for Phantom games}, 
  year={2015},
  volume={},
  number={},
  pages={267-274},
  keywords={Games;Phantoms;Monte Carlo methods;Mathematical model;Bismuth;Law},
  doi={10.1109/CIG.2015.7317917}}


@inproceedings{Brown2020rebel,
 author = {Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {17057--17069},
 publisher = {Curran Associates, Inc.},
 title = {Combining Deep Reinforcement Learning and Search for Imperfect-Information Games},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf},
 volume = {33},
 year = {2020}
}